[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ligea Alexander",
    "section": "",
    "text": "Ligea is a data analyst with 10+ years experience. She is passionate about utilizing her skills in data analysis, visualization, and storytelling to investigate and illustrate complex social, political, and economic issues.\n\n\nExperienceSkillsAdditional Skills\n\n\nFreelance | Data Journalist | 2024 - present\nFreelance | Writer | 2023 - present\nCenter for Employment Opportunities | Data Insights Specialist | 2019 - present\n\n\nArtificial Intelligence\nData Analysis\nData Visualization\nPython | pandas, numpy\nR | tidy,\nJavaScript | p5.js,\nData Visualization & Storytelling → Not just static charts, but interactive and engaging experiences. Insight-Driven Journalism & Research → Making sense of complex topics through data. UX & Information Design → Building tools and experiences that help people interpret information. Strategic Thinking → You don’t just analyze data; you consider its business and societal impact.\n\n\nSocial issues, community violence, gun violence, justice-involved, AI ethics\n\n\n\n\nEducation\n\n\nColumbia University | New York, NY  LEDE Program, Data Journalism| 2023\nMaryland Institute College of Art | Baltimore, MD  MPS Data Analytics and Visualization | 2020 - 2023"
  },
  {
    "objectID": "projects/blog.html",
    "href": "projects/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nData Extraction/ Scraping\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsight through Data\n\n\n\ninsights\n\n\npython\n\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMy D3 Journey\n\n\n\nd3.js\n\n\nobservable\n\n\ndata visualization\n\n\n\n\n\n\n\n\n\n\nFeb 13, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nRhetVIZ\n\n\n\ndata visualization\n\n\ntheory\n\n\n\n\n\n\n\n\n\n\nDec 3, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/all-work/pathway-pal/pathwaypal-chatbot.html",
    "href": "projects/all-work/pathway-pal/pathwaypal-chatbot.html",
    "title": "I Built An Enterprise ChatBot!",
    "section": "",
    "text": "Lorem ipsum Vivamus tincidunt nisi a suscipit consequat. Praesent eget convallis neque, ac scelerisque eros. Integer aliquet, elit et tempor convallis, sem libero tincidunt dolor, vitae tempor magna quam at nisi. Sed elementum ac tortor at fringilla. Aliquam et urna congue, tincidunt ipsum quis, congue nisl. Phasellus non sem massa.\nDonec quis interdum sem. Mauris ut arcu erat. Sed nisl tellus, consectetur ut tincidunt id, porttitor vel sapien. Pellentesque in lorem vitae sem euismod interdum. Phasellus fermentum consectetur velit. Duis purus diam, mattis vitae dui et, egestas tincidunt nisl. Pellentesque placerat fermentum purus eget pulvinar. Ut fringilla placerat leo, id luctus ipsum elementum non. Suspendisse accumsan metus non dolor blandit vehicula. Nulla sit amet sodales ipsum."
  },
  {
    "objectID": "projects/blog/rhetorical-visualizations/rhetviz.html",
    "href": "projects/blog/rhetorical-visualizations/rhetviz.html",
    "title": "RhetVIZ",
    "section": "",
    "text": "Start with the Audience\nWhen I attended MICA, whenever we would be instructed to work on a data visualization assignment, the first thing that we’d promoted to do was to think and ask “who is your audience” The audience paints the narrative. I started my data visualization career while working at a non-profit that was always careful about what was communicated. As I continued to advance in my data viz career, I saw the evolution in just how important data visualization and information design professionals are.\nThis past summer, I had the opportunity to join Enrico Bertini’s inaugural Rhetorical Visualization course. Over five weeks, our cohort explored the theory and practice of creating visualizations that don’t just inform but resonate with their intended audience. Through weekly discussions, hands-on exercises, and thought-provoking lessons, we unpacked the nuances of how visualizations convey messages and shape interpretations.\nRhetorical visualization goes beyond aesthetics or clarity—it’s about the messaging. As Enrico puts it, it examines “the role in the messages a visualization conveys and the inferences one can make.” This idea—how our design decisions influence audience understanding—was a recurring theme throughout the course and challenged me to think more critically about my work.\nOne of the most impactful lessons for me was the concept of framing. As Enrico emphasized, data doesn’t speak for itself. This might sound surprising, especially since many of us have heard the phrase tossed around casually in workplaces or even in the media. But the act of visualizing data—choosing what to include, exclude, and how to present it—inevitably shapes a narrative. This realization was both humbling and empowering. Framing taught me that no visualization is ever completely neutral. Every chart carries the weight of its creator’s decisions, from the scale of the axes to the title we give it. Even something as seemingly straightforward as a bar chart can convey vastly different messages depending on these choices. This pushed me to think critically about my own work: What narrative am I shaping? And is it aligned with the truth I want to convey?\nThese lessons hit home when thinking about high-stakes visualizations—charts about climate change, election results, or pandemics like COVID-19. In these scenarios, where misinformation can have far-reaching consequences, rhetorical visualization becomes a critical skill. It’s not just about making data clear; it’s about ensuring it’s truthful and responsibly framed.\nEnrico’s course was, at its core, a call to action: to create data visualizations with integrity and to develop the habit of scrutinizing our work and that of others. As practitioners, we hold immense responsibility in shaping how people interpret data. This course sharpened my ability to reason with data effectively and reinforced the idea that there is always a frame—whether we acknowledge it or not.\nTaking this course felt like an exercise in due diligence for any data visualization professional—a deep dive into the ethics of our work. It was a reminder that every decision we make, from data collection to visual design, carries weight. By applying the principles of rhetorical visualization, we can create visuals that don’t just inform but resonate with integrity and purpose."
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#crafting-visual-narratives-for-corporate-strategy",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#crafting-visual-narratives-for-corporate-strategy",
    "title": "Insight through Data",
    "section": "Crafting Visual Narratives for Corporate Strategy",
    "text": "Crafting Visual Narratives for Corporate Strategy\n\n\n\n\n\n\nAll data in this case study is anonymized to protect confidentiality. Details have been altered, but the depicted processes and insights are accurately represented.\n\n\n\nIn my role as the Data Insights Specialist II on the Learning and Impact team at the Center for Employment Opportunities, I directed the overall strategy for each quarterly board report. I identified key performance indicators (KPIs) by analyzing data and aligning them with the strategic goals of the organization. I coordinated the narrative, deciding which insights to present, and shaped the story to highlight our achievements and areas for improvement. I’d communicate these insights through visualizations that were clear and impactful.\nHere, I’ll share a glimpse into how I’d transform raw data [usually obtained from various departments] into engaging visual stories that inform strategic and operational decisions. It’s all about turning complex data into clear, useful insights, even for those who aren’t familiar with data science or statistical jargon. \nEach quarter, I’d set these goals for myself:\n\nShow Org-Wide Performance: Ensure strategic alignment with fiscal year objectives.\nShare Our Strategy for Improvement: Highlight key initiatives and outcomes\nDelivering Actionable Insights: Provide clear, impactful information to the board."
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#wrangling-the-data",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#wrangling-the-data",
    "title": "Insight through Data",
    "section": "Wrangling the Data",
    "text": "Wrangling the Data\nAt the end of each quarter, I’d reach out to various departments to gather updates on programs and activities. As the Data Insights Specialist II, I stayed informed on organizational activities year-round, as our team was heavily involved in evaluations and research. Proactively engaging with pilots, program performance, evaluations, research, and analyses allowed me to know exactly what to ask for when compiling the board report, ensuring relevant insights were included.\nThe Data\nUsually, these updates came back to me as rudimentary figures, spreadsheets, or dashboards from which I’d pull data points. I would then download these into .csv files and process them using my favorite tool: pandas.\n\n\n\n\n\n\n\n\n\n\n\n(a) Via G-chat: Quick messages containing key data points or updates.\n\n\n\n\n\n\n\n\n\n\n\n(b) Through Google Sheets: More structured data in spreadsheets shared across departments.\n\n\n\n\n\n\n\n\n\n\n\n(c) Inside Emails: Often, data came embedded within the text of an email, requiring careful extraction and verification.\n\n\n\n\n\n\n\nFigure 1: Data Sharing Methods Across Departments; Illustrations by Kristin Sandgren-Jolain\n\n\n\nIn addition to incorporating relevant program updates, we also included findings from the Learning and Impact team’s internal analyses. One quarter, we included insights derived from an evaluation of one of our intervention methods. These insights revealed specific characteristics and intervention dosages that significantly increased the likelihood of success for our participants.\n“Likelihood”—there’s a term you’ll often hear in data science. Within the Learning and Impact team and in discussions across departments, terms like probabilities and likelihoods were straightforward to discuss, thanks to the context we could provide. However, translating these and other statistical terms accurately into language and visuals for the board—without the benefit of interactive dialogue—posed a unique challenge.\nIn the code blocks that follow, which I’ve anonymized for confidentiality, I’ll briefly demonstrate how I transformed data into visuals for our board updates. The data cleaning, processing, and manipulation steps shown below are routine practices I used to prepare the data points received from various departments. These steps were crucial for converting raw data, whether received through G-chat, Google Sheets, or emails, into clear, impactful board-level visual presentations.\n\n# Setting Up Environment\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Read in Data\ndf1 = pd.read_csv(\"anonymized_all.csv\")\ndf2 = pd.read_csv(\"anonymized_fy24_only.csv\")\n\n\n# First few rows of raw data\ndf1.head()\ndf2.head()\n\n\n\n\n\n\n\n\nUnique Identifier\nStart Date\nClient Name\nJob ID\n\n\n\n\n0\n581013\n9/25/23\n581013 Record\njob-193451\n\n\n1\n583586\n7/17/23\n583586 Record\njob-198299\n\n\n2\n585047\n7/11/23\n585047 Record\njob-198331\n\n\n3\n576444\n7/9/23\n576444 Record\njob-198365\n\n\n4\n585401\n9/18/23\n585401 Record\njob-198436\n\n\n\n\n\n\n\n\n# Cleaning and transforming the data\ndef clean_df(df, date_col, drop_col):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.drop([drop_col], axis = 1) # Drops Unnecessary Columns\n    df.columns = df.columns.str.lower() # Standardize Column Names\n    \n    return df\n\ncleaned_df1 = clean_df(df1,'Start Date', 'Client Name')\ncleaned_df2 = clean_df(df2,'Start Date', 'Client Name')\n\n\ndef fiscal_year(date): # Fiscal Year Calculation\n    if date.month &gt;=7:\n        return date.year + 1\n    else:\n        return date.year\n    \ncleaned_df1['fiscal year'] = cleaned_df1['start date'].apply(fiscal_year)\ncleaned_df2['fiscal year'] = cleaned_df2['start date'].apply(fiscal_year)\n\n\n# Check FYs in dataset\ncleaned_df1['fiscal year'].unique()\n\narray([2021, 2022, 2023, 2024])\n\n\n\ncleaned_previous = cleaned_df1[cleaned_df1['fiscal year'].isin([2024])]\n# cleaned_previous.head()\n\n\nprevious_weekly_performance = (\n  cleaned_previous\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'previous_weekly_performance')\n)\n# previous_weekly_performance\n\n\nprevious_weekly_performance['cumulative_performance'] = (\n  previous_weekly_performance\n  .groupby('fiscal year')['previous_weekly_performance']\n  .cumsum()\n)\nprevious_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\ncumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n\n\n2\n2024\n2023-07-16\n99\n191\n\n\n3\n2024\n2023-07-23\n71\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n\n\n\n\n\n\n\n\ncurrent_weekly_performance = (\n  cleaned_df2\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'current_weekly_performance')\n)\ncurrent_weekly_performance['current_cumulative_performance']= (\n  current_weekly_performance\n  .groupby('fiscal year')['current_weekly_performance']\n  .cumsum()\n)\n# current_weekly_performance.head()\n\n\nmerged_df = pd.merge(previous_weekly_performance, current_weekly_performance, how = 'outer')\n# merged_df.head()"
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#designing-clear-visuals",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#designing-clear-visuals",
    "title": "Insight through Data",
    "section": "Designing Clear Visuals",
    "text": "Designing Clear Visuals\nAfter extensive data manipulation—cleaning, pivoting, and generating initial visuals like bar charts—I begin to see potential ways to make the data understandable for the board.\nFor the quarterly board presentation that used the anonymized code blocks introduced earlier, I constructed the two charts below using the resulting merged_df. Given the available variables, focusing on temporal dynamics and comparative metrics provides a valuable direction to uncover significant trends and irregularities to highlight in the final product.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The bar chart shows weekly performance trends for previous and current periods, highlighting fluctuations and potential seasonal patterns. This dense representation helps identify key variations but can be visually complex.\n\n\n\n\n\n\n\n\n\n\n\n(b) The line chart offers a clearer view of performance over time by smoothing out weekly noise and emphasizing long-term trends.\n\n\n\n\n\n\n\nFigure 2: Initial stages of the data visualization workflow, creating basic plots to explore and refine key trends\n\n\n\nSource: 1-intervention-analysis.ipynb\nNow, with some initial visualizations created, I can focus on refining and enhancing the clarity of the data presentation and insight delivery. One best practice I adhere to in my visualization work is the “30-second rule” (sometimes it’s more like 2 minutes). The idea is that viewers should quickly grasp the intent of a chart or graph. This contrasts with the more abstract visualizations by my favoured artists like Federica Fragipane and Alberto Cairo, who target a different audience.\nFor board presentations, I prioritize straightforward designs and avoid overly complex or abstract visuals. Sketching with pencil and paper after extensive screen time helps me push the boundaries of basic charts just enough to enhance clarity without becoming too abstract. This physical connection with my work makes the creative process more tangible and refreshing. It allows me to identify where strategic annotations, adjustments to bar charts, or repositioning elements in a scatterplot can significantly improve the visual clarity and effectiveness.\n\n\n\n\n\n\n\n\n\nSketch 1: An early concept for communicating demographic breakdown.\n\n\n\n\n\n\n\nSketch 2: A rough layout; I use simple lines and directional arrows to indicate movement.\n\n\n\n\n\n\n\nSketch 3: When clarity and speed are key, humbling myself to use versatile bar charts is best.\n\n\n\n\n\nWith these principles in mind I refine and enhance the clarity of the data presentation to produce final visuals."
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#final-product",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#final-product",
    "title": "Insight through Data",
    "section": "Final Product",
    "text": "Final Product\nBelow are some examples of the final visualizations I have created for corporate board presentations. These visuals are drawn from various projects, each tailored to address specific strategic questions. From demonstrating engagement levels across regions to highlighting the efficacy of strategic efforts over time, these visuals represent the culmination of thorough data analysis and creative design.\n\n\n\n\nAll data shown is illustrative and has been modified to maintain confidentiality.\n\n\n\n\nIntervention Visualization: This chart illustrates regional engagement levels following strategic interventions. Each number represents the number interactions per intervention type aimed at enhancing interaction effectiveness.\n\n\n\n\n\n\n\nPerformance Trend Chart The line graph depicts the effectiveness of strategic efforts, showing a notable increase in outcomes following specific interventions. Key moments, like the significant improvement marked in the chart, are pivotal discussion points in strategy meetings.\n\n\nThanks for reading :)\n\n\nNext Post"
  },
  {
    "objectID": "projects/blog/finally_d3/working_with_d3.html",
    "href": "projects/blog/finally_d3/working_with_d3.html",
    "title": "My D3 Journey",
    "section": "",
    "text": "In my recent deep dive into the German political landscape (which you can read about here), I worked with electoral data to create my first D3.js charts- marking the beginning of my hands-on experience with D3. I want to share the process behind it.\nThis is not a step-by-step tutorial (well, maybe parts of it will be useful to someone just starting with D3— or not). It’s really just me documenting my process. And hey, maybe a year from now, I’ll look back at my code, shake my head and wonder:\nAt least I’ll have proof of my D3.js baby steps— before I (hopefully) evolve into a D3 and data viz engineering beast."
  },
  {
    "objectID": "projects/blog/finally_d3/working_with_d3.html#choosing-the-right-charts",
    "href": "projects/blog/finally_d3/working_with_d3.html#choosing-the-right-charts",
    "title": "My D3 Journey",
    "section": "Choosing the Right Charts",
    "text": "Choosing the Right Charts\nSince I’m trying to decode Germany’s political landscape, I decided to visualize:\n\nVote share trends over time → To see how party popularity has shifted across different elections.\nA choropleth map → To understand how support for different parties varies across German states.\n\nEach of these charts will build on the dataset I processed earlier, with some tweaks along the way. Let’s dive in."
  },
  {
    "objectID": "projects/all-projects.html",
    "href": "projects/all-projects.html",
    "title": "My Projects",
    "section": "",
    "text": "I Built An Enterprise ChatBot!\n        \n\n        \n          \n          \n            \n              \n            \n          \n          \n      \n\n      \n      \n        \n          \n        \n        \n\n          \n          \n            An interactive tool that offers users answers to common questions about training schedules, services, and resources available through NEST.\n          \n\n          \n          \n            \n              \n                \n                  Python\n                  \n                  Elasticsearch\n                  \n                  AI\n                  \n              \n              \n          \n\n          \n          \n            Read Behind The Scenes (BTS)\n          \n    \n\n    \n\n    \n    \n      \n      \n        \n          The Cost of Sunlight\n        \n\n        \n          \n          \n            \n              \n            \n          \n          \n      \n\n      \n      \n        \n          \n        \n        \n\n          \n          \n            LEDE Project 2: A cost-benefit analysis of solar energy adoption.\n          \n\n          \n          \n            \n              \n                \n                  Python\n                  \n                  Altair\n                  \n                  ai2html\n                  \n              \n              \n          \n\n          \n          \n            Read Behind The Scenes (BTS)\n          \n    \n\n    \n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/blog/finally_d3/data/analysis.html",
    "href": "projects/blog/finally_d3/data/analysis.html",
    "title": "Data Extraction/ Scraping",
    "section": "",
    "text": "import pandas as pd\ndf = pd.read_csv(\"federal_cty_harm.csv\")\ndf.head()\n\n\n\n\n\n\n\n\ncounty_code\nelection_year\nstate\neligible_voters\nnumber_voters\nvalid_votes\ninvalid_votes\nturnout\ncdu\ncsu\n...\nvolt\nzentrum\narea\npopulation\nflag_unsuccessful_naive_merge\ntotal_votes\ncdu_csu\nfar_right\nfar_left\nfar_left_w_linke\n\n\n\n\n0\n1001\n1990\n1\n69563\n50485\n50036\n449\n0.725745\n0.370533\n0.0\n...\n0.0\n0.0\n56.36\n86.6\nNaN\n50036\n0.370533\nNaN\nNaN\nNaN\n\n\n1\n1001\n1994\n1\n68987\n52379\n51823\n556\n0.759259\n0.344191\n0.0\n...\n0.0\n0.0\n56.44\n87.9\nNaN\n51823\n0.344191\nNaN\nNaN\nNaN\n\n\n2\n1001\n1998\n1\n65755\n50560\n49862\n698\n0.768915\n0.291565\n0.0\n...\n0.0\n0.0\n56.44\n84.7\nNaN\n49862\n0.291565\nNaN\nNaN\nNaN\n\n\n3\n1001\n2002\n1\n65740\n49054\n48553\n501\n0.746182\n0.295800\n0.0\n...\n0.0\n0.0\n56.38\n84.7\nNaN\n48553\n0.295800\nNaN\nNaN\nNaN\n\n\n4\n1001\n2005\n1\n66970\n49002\n48235\n767\n0.731701\n0.288380\n0.0\n...\n0.0\n0.0\n56.38\n86.1\nNaN\n48235\n0.288380\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 120 columns\ndf.dtypes\n\ncounty_code           int64\nelection_year         int64\nstate                 int64\neligible_voters       int64\nnumber_voters         int64\n                     ...   \ntotal_votes           int64\ncdu_csu             float64\nfar_right           float64\nfar_left            float64\nfar_left_w_linke    float64\nLength: 120, dtype: object\n# import requests\n# from bs4 import BeautifulSoup\n# elections_data = requests.get(\"https://www.wahlrecht.de/ergebnisse/hamburg.htm\").text\n# print(elections_data)\n# soup = BeautifulSoup(elections_data)\n# soup.head.title\n# hh_table = soup.select(\"table\")\n# len(hh_table)\n# hh_table[1]\n# hh_table = soup.select(\"table.border\")\n# hh_tables = hh_table[0]\n# hh_tables.text[:200]\n# row_els = hh_tables.select(\"tbody tr\")\n# len(row_els)\n# row_els[1]\n# [cell.text for cell in row_els[0].select(\"td\")]\n# import nest_asyncio\n    # import asyncio\n    # from playwright.async_api import async_playwright\n    # import pandas as pd\n    # from bs4 import BeautifulSoup\n# # Enable async in Jupyter\n# nest_asyncio.apply()\n\n# BASE_URL = \"https://www.wahlrecht.de/ergebnisse/index.htm\"\n# HOST_URL = \"https://www.wahlrecht.de/ergebnisse/\"\n\n# async def get_links_from_base():\n#     \"\"\"Fetch the base page and extract all links from the second table with class='border'.\"\"\"\n#     async with async_playwright() as p:\n#         browser = await p.chromium.launch(headless=True)\n#         page = await browser.new_page()\n#         await page.goto(BASE_URL)\n        \n#         # Get the HTML of the base page\n#         base_content = await page.content()\n#         soup = BeautifulSoup(base_content, \"html.parser\")\n        \n#         # Find all tables with class='border'\n#         tables = soup.find_all(\"table\", class_=\"border\")\n        \n#         if len(tables) &lt; 2:\n#             print(\"Could not find the second table with class='border'.\")\n#             await browser.close()\n#             return []\n        \n#         # Get the second table\n#         second_table = tables[1]\n        \n#         # In the second table, find the FIRST &lt;tbody&gt;\n#         tbody = second_table.find(\"tbody\")\n#         if not tbody:\n#             print(\"No &lt;tbody&gt; found in the second table.\")\n#             await browser.close()\n#             return []\n        \n#         # Extract links from the rows\n#         links = []\n#         for tr in tbody.find_all(\"tr\"):\n#             # Check each cell for &lt;a&gt; tags\n#             for td in tr.find_all(\"td\"):\n#                 a_tag = td.find(\"a\", href=True)\n#                 if a_tag:\n#                     relative_href = a_tag[\"href\"]\n#                     # Convert relative URL to absolute\n#                     full_url = HOST_URL + relative_href\n#                     link_text = a_tag.get_text(strip=True)\n#                     links.append((link_text, full_url))\n        \n#         await browser.close()\n#         return links\n# def parse_complex_table(table_soup, source_name, table_index):\n#     \"\"\"\n#     Parse a table with a two-row &lt;thead&gt; structure that represents election results.\n    \n#     Expected structure (e.g., on thueringen.htm):\n#       &lt;thead&gt;\n#         &lt;tr&gt;\n#           &lt;th rowspan=\"2\"&gt;&lt;/th&gt;         &lt;!-- empty cell: becomes \"Party\" --&gt;\n#           &lt;th class=\"jahr\" colspan=\"2\"&gt;1990&lt;/th&gt;\n#           &lt;th class=\"jahr\" colspan=\"2\"&gt;1994&lt;/th&gt;\n#           ... etc.\n#         &lt;/tr&gt;\n#         &lt;tr&gt;\n#           &lt;th&gt;%&lt;/th&gt;\n#           &lt;th&gt;Sitze&lt;/th&gt;\n#           &lt;th&gt;%&lt;/th&gt;\n#           &lt;th&gt;Sitze&lt;/th&gt;\n#           ... etc.\n#         &lt;/tr&gt;\n#       &lt;/thead&gt;\n      \n#     The final MultiIndex for columns will be:\n#       [(\"Party\", \"\"), (\"1990\", \"%\"), (\"1990\", \"Sitze\"), (\"1994\", \"%\"), (\"1994\", \"Sitze\"), …]\n      \n#     The &lt;tbody&gt; should have rows with exactly (1 + 2*N) cells.\n#     Returns a DataFrame with MultiIndex columns or None if the structure is not as expected.\n#     \"\"\"\n#     thead = table_soup.find(\"thead\")\n#     if not thead:\n#         return None\n\n#     header_rows = thead.find_all(\"tr\")\n#     if len(header_rows) != 2:\n#         return None  # Expect exactly two header rows\n\n#     # ----- Row 0: Top-level headers -----\n#     # The first &lt;th&gt; (with rowspan=\"2\") should be empty and becomes the Party column header.\n#     row0_cells = header_rows[0].find_all(\"th\")\n#     if not row0_cells or len(row0_cells) &lt; 2:\n#         return None\n#     party_header = row0_cells[0].get_text(strip=True) or \"Party\"\n#     top_headers = [party_header]  # first column header\n\n#     # For the remaining cells, repeat each cell's text according to its colspan.\n#     for cell in row0_cells[1:]:\n#         try:\n#             colspan = int(cell.get(\"colspan\", \"1\"))\n#         except ValueError:\n#             colspan = 1\n#         text = cell.get_text(strip=True)\n#         for _ in range(colspan):\n#             top_headers.append(text)\n\n#     # ----- Row 1: Bottom-level headers (sub-headers) -----\n#     row1_cells = header_rows[1].find_all(\"th\")\n#     if not row1_cells:\n#         return None\n#     # For the Party column, we assign an empty string.\n#     bottom_headers = [\"\"]\n#     for cell in row1_cells:\n#         bottom_headers.append(cell.get_text(strip=True))\n\n#     if len(top_headers) != len(bottom_headers):\n#         return None\n\n#     # Build the MultiIndex for the DataFrame columns.\n#     columns = pd.MultiIndex.from_arrays([top_headers, bottom_headers])\n\n#     # ----- Parse the &lt;tbody&gt; -----\n#     tbody = table_soup.find(\"tbody\")\n#     if not tbody:\n#         return None\n\n#     data_rows = []\n#     for tr in tbody.find_all(\"tr\"):\n#         cells = tr.find_all([\"td\", \"th\"])\n#         row_values = [cell.get_text(strip=True) for cell in cells]\n#         if row_values:\n#             data_rows.append(row_values)\n\n#     # Only accept rows that match the number of header columns.\n#     expected_cols = len(columns)\n#     valid_rows = [row for row in data_rows if len(row) == expected_cols]\n#     if not valid_rows:\n#         return None\n\n#     df = pd.DataFrame(valid_rows, columns=columns)\n#     df.insert(0, \"Table_Index\", table_index)\n#     df.insert(0, \"Source_Page\", source_name)\n#     return df\n# def parse_complex_table(table_soup, source_name, table_index):\n#     \"\"\"\n#     Parse a table with a two-row &lt;thead&gt; structure that represents election results,\n#     building a MultiIndex header as follows:\n    \n#     Expected &lt;thead&gt; structure:\n#       &lt;thead&gt;\n#         &lt;tr&gt;\n#           &lt;th rowspan=\"2\"&gt;&lt;/th&gt;         &lt;!-- empty cell becomes \"Party\" --&gt;\n#           &lt;th class=\"jahr\" colspan=\"2\"&gt;1990&lt;/th&gt;\n#           &lt;th class=\"jahr\" colspan=\"2\"&gt;1994&lt;/th&gt;\n#           ... etc.\n#         &lt;/tr&gt;\n#         &lt;tr&gt;\n#           &lt;th&gt;%&lt;/th&gt;\n#           &lt;th&gt;Sitze&lt;/th&gt;\n#           &lt;th&gt;%&lt;/th&gt;\n#           &lt;th&gt;Sitze&lt;/th&gt;\n#           ... etc.\n#         &lt;/tr&gt;\n#       &lt;/thead&gt;\n      \n#     The final MultiIndex for columns will be:\n#       [(\"Party\", \"\"), (\"1990\", \"%\"), (\"1990\", \"Sitze\"), (\"1994\", \"%\"), (\"1994\", \"Sitze\"), …]\n      \n#     The data rows may be split across multiple &lt;tbody&gt; tags.\n#     Returns a DataFrame with MultiIndex columns or None if the structure is not as expected.\n#     \"\"\"\n#     # ----- Parse the header -----\n#     thead = table_soup.find(\"thead\")\n#     if not thead:\n#         return None  # No &lt;thead&gt; found; skip this table\n\n#     header_rows = thead.find_all(\"tr\")\n#     if len(header_rows) != 2:\n#         return None  # Expect exactly two header rows\n\n#     # Row 0: Top-level headers\n#     row0_cells = header_rows[0].find_all(\"th\")\n#     if not row0_cells or len(row0_cells) &lt; 2:\n#         return None\n#     party_header = row0_cells[0].get_text(strip=True) or \"Party\"\n#     top_headers = [party_header]\n#     for cell in row0_cells[1:]:\n#         try:\n#             colspan = int(cell.get(\"colspan\", \"1\"))\n#         except ValueError:\n#             colspan = 1\n#         text = cell.get_text(strip=True)\n#         for _ in range(colspan):\n#             top_headers.append(text)\n\n#     # Row 1: Sub-headers (lower-level headers)\n#     row1_cells = header_rows[1].find_all(\"th\")\n#     if not row1_cells:\n#         return None\n#     bottom_headers = [\"\"]\n#     for cell in row1_cells:\n#         bottom_headers.append(cell.get_text(strip=True))\n\n#     if len(top_headers) != len(bottom_headers):\n#         return None\n\n#     # Create the MultiIndex columns\n#     columns = pd.MultiIndex.from_arrays([top_headers, bottom_headers])\n\n#     # ----- Parse the data from all &lt;tbody&gt; tags -----\n#     tbodys = table_soup.find_all(\"tbody\")\n#     if not tbodys:\n#         return None\n\n#     data_rows = []\n#     for tbody in tbodys:\n#         for tr in tbody.find_all(\"tr\"):\n#             cells = tr.find_all([\"td\", \"th\"])\n#             row_values = [cell.get_text(strip=True) for cell in cells]\n#             if row_values:\n#                 data_rows.append(row_values)\n\n#     # Only keep rows that have the expected number of columns.\n#     expected_cols = len(columns)\n#     valid_rows = [row for row in data_rows if len(row) == expected_cols]\n#     if not valid_rows:\n#         return None\n\n#     df = pd.DataFrame(valid_rows, columns=columns)\n#     df.insert(0, \"Table_Index\", table_index)\n#     df.insert(0, \"Source_Page\", source_name)\n#     return df\n# async def scrape_page_tables(url, name):\n#     \"\"\"\n#     Scrape all &lt;table&gt; tags from a given URL.\n#     We'll try parse_complex_table() first.\n#     If that fails (None), we'll fallback to the naive approach.\n#     \"\"\"\n#     async with async_playwright() as p:\n#         browser = await p.chromium.launch(headless=True)\n#         page = await browser.new_page()\n#         await page.goto(url)\n        \n#         # Get page HTML\n#         html = await page.content()\n#         soup = BeautifulSoup(html, \"html.parser\")\n        \n#         # Find all &lt;table&gt;\n#         tables = soup.find_all(\"table\")\n        \n#         dfs = []\n#         for i, table in enumerate(tables):\n#             # Try complex header parsing\n#             df_complex = parse_complex_table(table, name, i)  # &lt;-- uses the new version\n#             if df_complex is not None:\n#                 # If we successfully parsed a complex header, use that\n#                 dfs.append(df_complex)\n#             else:\n#                 # Fallback: naive approach\n#                 rows = table.find_all(\"tr\")\n#                 data = []\n#                 # Attempt to detect headers in the first row if it has &lt;th&gt;\n#                 if rows and rows[0].find(\"th\"):\n#                     headers = [th.get_text(strip=True) for th in rows[0].find_all(\"th\")]\n#                     data_start_idx = 1\n#                 else:\n#                     headers = None\n#                     data_start_idx = 0\n\n#                 # Extract rows\n#                 for row in rows[data_start_idx:]:\n#                     cols = [td.get_text(strip=True) for td in row.find_all(\"td\")]\n#                     if cols:\n#                         data.append(cols)\n\n#                 max_cols = max((len(r) for r in data), default=0)\n#                 if headers and len(headers) != max_cols:\n#                     headers = None\n\n#                 if data:\n#                     df = pd.DataFrame(data, columns=headers if headers else None)\n#                     df.insert(0, \"Table_Index\", i)\n#                     df.insert(0, \"Source_Page\", name)\n#                     dfs.append(df)\n\n#         await browser.close()\n#         return dfs\n# # ... [rest of your code remains unchanged]\n\n# async def main():\n#     \"\"\"Main flow: get links, then scrape each page's tables.\"\"\"\n#     links = await get_links_from_base()\n#     if not links:\n#         print(\"No links found from the base page.\")\n#         return\n    \n#     print(f\"Found {len(links)} links from base page.\")\n    \n#     all_results = {}\n#     for link_text, link_url in links:\n#         print(f\"\\nScraping: {link_text} -&gt; {link_url}\")\n#         page_dfs = await scrape_page_tables(link_url, link_text)\n#         if page_dfs:\n#             all_results[link_text] = page_dfs\n\n#     # Save/Display only the tables that meet our column threshold\n#     min_columns = 6  # change this number as appropriate for your complex tables\n#     for state_name, dfs in all_results.items():\n#         for idx, df in enumerate(dfs):\n#             if df.shape[1] &lt; min_columns:\n#                 # Skip tables that are too \"simple\"\n#                 continue\n\n#             print(f\"\\nDataFrame from {state_name} (Table {idx}):\")\n#             display(df.head())\n#             csv_name = f\"{state_name.replace(' ', '_')}_table_{idx}.csv\"\n#             df.to_csv(csv_name, index=False)\n#             print(f\"Saved: {csv_name}\")\n\n# # Run in Jupyter\n# await main()"
  },
  {
    "objectID": "projects/blog/finally_d3/data/analysis.html#data-transformation-cleaning",
    "href": "projects/blog/finally_d3/data/analysis.html#data-transformation-cleaning",
    "title": "Data Extraction/ Scraping",
    "section": "Data Transformation & Cleaning",
    "text": "Data Transformation & Cleaning\n\nimport glob\n\n\n# Path to your CSV folder\ncsv_folder = \"state_election_results/*.csv\"\n\n# Find all CSV files\nfiles = glob.glob(csv_folder)\n\nall_long_dfs = []\n\nfor file in files:\n    df = pd.read_csv(file)\n    \n    # The first 3 columns are \"Source_Page\", \"Table_Index\", \"Party\"\n    fixed_cols = df.columns[:3]  # e.g. ['Source_Page', 'Table_Index', 'Party']\n    # The rest are the repeated year columns\n    repeated_years = df.columns[3:]\n    \n    # Rename the repeated columns in pairs: \n    # e.g. \"1952\", \"1952\" -&gt; \"1952_%\", \"1952_Sitze\"\n    new_cols = list(fixed_cols)  # start with the first 3 columns as-is\n    \n    # Make sure we have an even number of columns in repeated_years\n    # If it's odd, there's a leftover column you might need to handle or skip.\n    if len(repeated_years) % 2 != 0:\n        print(f\"Warning: {file} has an odd number of repeated columns. Check manually!\")\n    \n    # Build new column names in pairs\n    for i in range(0, len(repeated_years), 2):\n        year_col_1 = repeated_years[i]\n        # The second column in the pair:\n        if i+1 &lt; len(repeated_years):\n            year_col_2 = repeated_years[i+1]\n        else:\n            year_col_2 = None\n        \n        # We assume both are the same year label (like \"1952\", \"1952\"),\n        # so we can get the year from the first\n        year_label = year_col_1\n        \n        # Create new names\n        new_cols.append(f\"{year_label}_%\")\n        new_cols.append(f\"{year_label}_Sitze\")\n    \n    # Assign the new column names to df\n    df.columns = new_cols\n    \n    # Now \"df\" has columns like:\n    # Source_Page, Table_Index, Party, 1952_%, 1952_Sitze, 1956_%, 1956_Sitze, ...\n    \n    # Melt (unpivot) from wide to long\n    # id_vars = the columns we don't want to melt\n    id_vars = [\"Source_Page\", \"Table_Index\", \"Party\"]\n    value_vars = df.columns[len(id_vars):]  # all the year_% and year_Sitze columns\n    \n    long_df = pd.melt(\n        df,\n        id_vars=id_vars,\n        value_vars=value_vars,\n        var_name=\"Year_Stat\",\n        value_name=\"Value\"\n    )\n    \n    # \"Year_Stat\" might look like \"1952_%\", \"1952_Sitze\"\n    # Split into \"Year\" and \"StatType\"\n    long_df[[\"Year\", \"StatType\"]] = long_df[\"Year_Stat\"].str.split(\"_\", expand=True)\n    \n    # Keep only the relevant columns\n    long_df = long_df[[\"Source_Page\", \"Party\", \"Year\", \"StatType\", \"Value\"]]\n    \n    # Append to list\n    all_long_dfs.append(long_df)\n\n\n# Concatenate all long dataframes\nfull_long_df = pd.concat(all_long_dfs, ignore_index=True)\nfull_long_df.columns = full_long_df.columns.str.lower()\nfull_long_df.columns\n\nIndex(['source_page', 'party', 'year', 'stattype', 'value'], dtype='object')\n\n\n\nfull_long_df\n\n\n\n\n\n\n\n\nsource_page\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nNaN\nNaN\n1946\n%\n%\n\n\n1\nHessen\nWahlbe­teiligung\n1946\n%\n73,2\n\n\n2\nHessen\nCDU\n1946\n%\n30,9\n\n\n3\nHessen\nSPD\n1946\n%\n42,7\n\n\n4\nHessen\nFDP\n1946\n%\n15,7\n\n\n...\n...\n...\n...\n...\n...\n\n\n8243\nSchleswig-Holstein\nDP³\n2022\nSitze\n–\n\n\n8244\nSchleswig-Holstein\nKPD/DKP¹\n2022\nSitze\n–\n\n\n8245\nSchleswig-Holstein\nREP\n2022\nSitze\n–\n\n\n8246\nSchleswig-Holstein\nDVU\n2022\nSitze\n–\n\n\n8247\nSchleswig-Holstein\nSonstige\n2022\nSitze\n–\n\n\n\n\n8248 rows × 5 columns\n\n\n\n\nfull_long_df[full_long_df[\"source_page\"].isna()]\n\n\n\n\n\n\n\n\nsource_page\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nNaN\nNaN\n1946\n%\n%\n\n\n14\nNaN\nNaN\n1946\nSitze\nSitze\n\n\n28\nNaN\nNaN\n1950\n%\n%\n\n\n42\nNaN\nNaN\n1950\nSitze\nSitze\n\n\n56\nNaN\nNaN\n1954\n%\n%\n\n\n...\n...\n...\n...\n...\n...\n\n\n8163\nNaN\nNaN\n2012\nSitze\nSitze\n\n\n8180\nNaN\nNaN\n2017\n%\n%\n\n\n8197\nNaN\nNaN\n2017\nSitze\nSitze\n\n\n8214\nNaN\nNaN\n2022\n%\n%\n\n\n8231\nNaN\nNaN\n2022\nSitze\nSitze\n\n\n\n\n504 rows × 5 columns\n\n\n\n\ncleaned_full_long_df = full_long_df.dropna(subset=[\"source_page\"]).reset_index(drop=True)\ncleaned_full_long_df\n\n\n\n\n\n\n\n\nsource_page\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nHessen\nWahlbe­teiligung\n1946\n%\n73,2\n\n\n1\nHessen\nCDU\n1946\n%\n30,9\n\n\n2\nHessen\nSPD\n1946\n%\n42,7\n\n\n3\nHessen\nFDP\n1946\n%\n15,7\n\n\n4\nHessen\nGRÜNE\n1946\n%\n–\n\n\n...\n...\n...\n...\n...\n...\n\n\n7739\nSchleswig-Holstein\nDP³\n2022\nSitze\n–\n\n\n7740\nSchleswig-Holstein\nKPD/DKP¹\n2022\nSitze\n–\n\n\n7741\nSchleswig-Holstein\nREP\n2022\nSitze\n–\n\n\n7742\nSchleswig-Holstein\nDVU\n2022\nSitze\n–\n\n\n7743\nSchleswig-Holstein\nSonstige\n2022\nSitze\n–\n\n\n\n\n7744 rows × 5 columns\n\n\n\n\ncleaned_full_long_df = cleaned_full_long_df.rename(columns={'source_page': 'state'})\ncleaned_full_long_df\n\n\n\n\n\n\n\n\nstate\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nHessen\nWahlbe­teiligung\n1946\n%\n73,2\n\n\n1\nHessen\nCDU\n1946\n%\n30,9\n\n\n2\nHessen\nSPD\n1946\n%\n42,7\n\n\n3\nHessen\nFDP\n1946\n%\n15,7\n\n\n4\nHessen\nGRÜNE\n1946\n%\n–\n\n\n...\n...\n...\n...\n...\n...\n\n\n7739\nSchleswig-Holstein\nDP³\n2022\nSitze\n–\n\n\n7740\nSchleswig-Holstein\nKPD/DKP¹\n2022\nSitze\n–\n\n\n7741\nSchleswig-Holstein\nREP\n2022\nSitze\n–\n\n\n7742\nSchleswig-Holstein\nDVU\n2022\nSitze\n–\n\n\n7743\nSchleswig-Holstein\nSonstige\n2022\nSitze\n–\n\n\n\n\n7744 rows × 5 columns\n\n\n\n\n# Create a mask for rows where stattype is '%'\nmask = cleaned_full_long_df['stattype'] == '%'\n\n# Replace comma with period and convert to numeric\ncleaned_full_long_df.loc[mask, 'value'] = cleaned_full_long_df.loc[mask, 'value'].str.replace(',', '.')\ncleaned_full_long_df.loc[mask, 'value'] = pd.to_numeric(cleaned_full_long_df.loc[mask, 'value'], errors='coerce') / 100\n\n\n# Filter for voter turnout rows\nturnout_df = cleaned_full_long_df[cleaned_full_long_df['party'] == 'Wahlbe­teiligung'].copy()\n\n# Ensure the 'year' column is numeric so that sorting works correctly.\nturnout_df['year'] = pd.to_numeric(turnout_df['year'], errors='coerce')\n\n# Pivot the data so that each (state, year) row gets both \"%\" and \"Sitze\" values\nturnout_table = turnout_df.pivot_table(\n    index=[\"state\", \"year\"],\n    columns=\"stattype\",\n    values=\"value\",\n    aggfunc=\"first\"  # if there are duplicates, you can use \"mean\" or another aggregator\n).reset_index()\n\n# Remove the name for the columns index and rename the turnout columns for clarity\nturnout_table.columns.name = None\nturnout_table = turnout_table.rename(columns={\"%\": \"Turnout_Percentage\", \"Sitze\": \"Turnout_Sitze\"})\n\n# Optionally, sort the table by state and year\nturnout_table = turnout_table.sort_values(by=[\"state\", \"year\"])\n\nturnout_table.head()\n\n\n\n\n\n\n\n\nstate\nyear\nTurnout_Percentage\nTurnout_Sitze\n\n\n\n\n0\nBaden-Württemberg\n1952\n0.637\n121\n\n\n1\nBaden-Württemberg\n1956\n0.703\n120\n\n\n2\nBaden-Württemberg\n1960\n0.59\n121\n\n\n3\nBaden-Württemberg\n1964\n0.677\n120\n\n\n4\nBaden-Württemberg\n1968\n0.7\n127\n\n\n\n\n\n\n\n\ncleaned_full_long_df = cleaned_full_long_df[cleaned_full_long_df[\"party\"] != \"Wahlbe­teiligung\"].reset_index(drop=True)\n\n\ncleaned_full_long_df\n\n\n\n\n\n\n\n\nstate\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nHessen\nCDU\n1946\n%\n0.309\n\n\n1\nHessen\nSPD\n1946\n%\n0.427\n\n\n2\nHessen\nFDP\n1946\n%\n0.157\n\n\n3\nHessen\nGRÜNE\n1946\n%\nNaN\n\n\n4\nHessen\nDIE LINKE\n1946\n%\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n7515\nSchleswig-Holstein\nDP³\n2022\nSitze\n–\n\n\n7516\nSchleswig-Holstein\nKPD/DKP¹\n2022\nSitze\n–\n\n\n7517\nSchleswig-Holstein\nREP\n2022\nSitze\n–\n\n\n7518\nSchleswig-Holstein\nDVU\n2022\nSitze\n–\n\n\n7519\nSchleswig-Holstein\nSonstige\n2022\nSitze\n–\n\n\n\n\n7520 rows × 5 columns\n\n\n\n\ncleaned_full_long_df[cleaned_full_long_df[\"party\"]==\"Wahlbe­teiligung\"]\n\n\n\n\n\n\n\n\nstate\nparty\nyear\nstattype\nvalue\n\n\n\n\n\n\n\n\n\n\n# cleaned_full_long_df.to_csv(\"test.csv\", index = False)\n\n\npercent_votes = cleaned_full_long_df[cleaned_full_long_df[\"stattype\"]==\"%\"].reset_index(drop = True)\npercent_votes\n\n# percent_votes.to_csv(\"percentage_votes.csv\", index = False)\n\n\n\n\n\n\n\n\nstate\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nHessen\nCDU\n1946\n%\n0.309\n\n\n1\nHessen\nSPD\n1946\n%\n0.427\n\n\n2\nHessen\nFDP\n1946\n%\n0.157\n\n\n3\nHessen\nGRÜNE\n1946\n%\nNaN\n\n\n4\nHessen\nDIE LINKE\n1946\n%\nNaN\n\n\n...\n...\n...\n...\n...\n...\n\n\n3755\nSchleswig-Holstein\nDP³\n2022\n%\nNaN\n\n\n3756\nSchleswig-Holstein\nKPD/DKP¹\n2022\n%\nNaN\n\n\n3757\nSchleswig-Holstein\nREP\n2022\n%\nNaN\n\n\n3758\nSchleswig-Holstein\nDVU\n2022\n%\nNaN\n\n\n3759\nSchleswig-Holstein\nSonstige\n2022\n%\n0.038\n\n\n\n\n3760 rows × 5 columns\n\n\n\n\nseats_won = cleaned_full_long_df[cleaned_full_long_df[\"stattype\"]==\"Sitze\"].reset_index(drop = True)\nseats_won\n# seats_won.to_csv(\"seats_won.csv\", index = False)\n\n\n\n\n\n\n\n\nstate\nparty\nyear\nstattype\nvalue\n\n\n\n\n0\nHessen\nCDU\n1946\nSitze\n28\n\n\n1\nHessen\nSPD\n1946\nSitze\n38\n\n\n2\nHessen\nFDP\n1946\nSitze\n14\n\n\n3\nHessen\nGRÜNE\n1946\nSitze\n–\n\n\n4\nHessen\nDIE LINKE\n1946\nSitze\n–\n\n\n...\n...\n...\n...\n...\n...\n\n\n3755\nSchleswig-Holstein\nDP³\n2022\nSitze\n–\n\n\n3756\nSchleswig-Holstein\nKPD/DKP¹\n2022\nSitze\n–\n\n\n3757\nSchleswig-Holstein\nREP\n2022\nSitze\n–\n\n\n3758\nSchleswig-Holstein\nDVU\n2022\nSitze\n–\n\n\n3759\nSchleswig-Holstein\nSonstige\n2022\nSitze\n–\n\n\n\n\n3760 rows × 5 columns"
  },
  {
    "objectID": "projects/blog/finally_d3/data/analysis.html#analysis",
    "href": "projects/blog/finally_d3/data/analysis.html#analysis",
    "title": "Data Extraction/ Scraping",
    "section": "Analysis",
    "text": "Analysis"
  },
  {
    "objectID": "projects/blog/corporate-visualizations/1-intervention-analysis.html",
    "href": "projects/blog/corporate-visualizations/1-intervention-analysis.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf1 = pd.read_csv(\"anonymized_all.csv\")\ndf2 = pd.read_csv(\"anonymized_fy24_only.csv\")\n\n\ndf1.head()\n\n\n\n\n\n\n\n\nUnique Identifier\nStart Date\nClient Name\nJob ID\n\n\n\n\n0\n537926\n6/14/21\n537926 Record\njob-188418\n\n\n1\n476870\n6/18/21\n476870 Record\njob-188751\n\n\n2\n458255\n6/14/21\n458255 Record\njob-189026\n\n\n3\n479256\n8/4/20\n479256 Record\njob-181914\n\n\n4\n472697\n8/7/20\n472697 Record\njob-181916\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nUnique Identifier\nStart Date\nClient Name\nJob ID\n\n\n\n\n0\n581013\n9/25/23\n581013 Record\njob-193451\n\n\n1\n583586\n7/17/23\n583586 Record\njob-198299\n\n\n2\n585047\n7/11/23\n585047 Record\njob-198331\n\n\n3\n576444\n7/9/23\n576444 Record\njob-198365\n\n\n4\n585401\n9/18/23\n585401 Record\njob-198436\n\n\n\n\n\n\n\n\ndef clean_df(df, date_col, drop_col):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.drop([drop_col], axis = 1)\n    df.columns = df.columns.str.lower()\n    \n    return df\n\ncleaned_df1 = clean_df(df1,'Start Date', 'Client Name')\ncleaned_df2 = clean_df(df2,'Start Date', 'Client Name')\n\n\ncleaned_df1.head()\n\n\n\n\n\n\n\n\nunique identifier\nstart date\njob id\n\n\n\n\n0\n537926\n2021-06-14\njob-188418\n\n\n1\n476870\n2021-06-18\njob-188751\n\n\n2\n458255\n2021-06-14\njob-189026\n\n\n3\n479256\n2020-08-04\njob-181914\n\n\n4\n472697\n2020-08-07\njob-181916\n\n\n\n\n\n\n\n\ndef fiscal_year(date):\n    if date.month &gt;=7:\n        return date.year + 1\n    else:\n        return date.year\n    \ncleaned_df1['fiscal year'] = cleaned_df1['start date'].apply(fiscal_year)\ncleaned_df2['fiscal year'] = cleaned_df2['start date'].apply(fiscal_year)\n\n\ncleaned_df1['fiscal year'].unique()\n\narray([2021, 2022, 2023, 2024])\n\n\n\ncleaned_df2['fiscal year'].unique()\n\narray([2024])\n\n\n\n# &lt;!-- ```{python}\n# merged_df = pd.merge(cleaned_all, cleaned_fy24, how = 'outer')\n# merged_df\n# ```\n\n\n\n# ```{python}\n# merged_df['fiscal year'] = merged_df['start date'].apply(get_fiscal_year)\n# merged_df.head()\n# ```\n# ```{python}\n# merged_df['fiscal year'].unique()\n# ```\n\n# ```{python}\n# merged_df = merged_df[merged_df['fiscal year'].isin([2023, 2024])]\n# merged_df\n# ```\n\n# ```{python}\n# weekly_performance = (\n#   merged_df\n#   .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n#   .reset_index(name = 'weekly_performance')\n# )\n\n# weekly_performance\n# ```\n\n# ```{python}\n# weekly_performance['cumulative_performance'] = (\n#   weekly_performance\n#   .groupby('fiscal year')['weekly_performance']\n#   .cumsum()\n# )\n\n# weekly_performance\n# ```\n# ```{python}\n# # Pivot the data\n# weekly_performance['start date'] = weekly_performance['start date'].dt.strftime('%m-%d')\n\n# pivoted_data = weekly_performance.pivot(index='start date', columns='fiscal year', values='cumulative_performance')\n# pivoted_data.columns = [f'Cumulative Performance FY{year}' for year in pivoted_data.columns]\n\n# pivoted_data.reset_index(inplace=True)\n\n# pivoted_data\n# ```\n# ```{python}\n# # Plotting\n# fig, ax = plt.subplots(figsize=(10, 6))\n# pivoted_data.plot(x='start date', ax=ax)\n\n# ax.set_title('Cumulative Weekly Performance by Fiscal Year')\n# ax.set_xlabel('Date')\n# ax.set_ylabel('Cumulative Performance')\n# ax.legend()\n\n# plt.show()\n# ```\n# ```{python}\n# weekly_performance.to_csv(\"weekly_performance.csv\", index = False) \n# ``` --&gt;\n\n\ncleaned_previous = cleaned_df1[cleaned_df1['fiscal year'].isin([2024])]\ncleaned_previous\n\n\n\n\n\n\n\n\nunique identifier\nstart date\njob id\nfiscal year\n\n\n\n\n8381\n581013\n2023-09-25\njob-193451\n2024\n\n\n8382\n583586\n2023-07-17\njob-198299\n2024\n\n\n8383\n585047\n2023-07-11\njob-198331\n2024\n\n\n8384\n576444\n2023-07-09\njob-198365\n2024\n\n\n8385\n585401\n2023-09-18\njob-198436\n2024\n\n\n...\n...\n...\n...\n...\n\n\n11009\n619903\n2024-03-13\njob-205994\n2024\n\n\n11010\n602062\n2024-03-26\njob-206010\n2024\n\n\n11011\n598480\n2024-03-06\njob-206013\n2024\n\n\n11012\n615498\n2024-03-03\njob-206017\n2024\n\n\n11013\n460343\n2024-02-11\njob-206027\n2024\n\n\n\n\n2633 rows × 4 columns\n\n\n\n\nprevious_weekly_performance = (\n  cleaned_previous\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'previous_weekly_performance')\n)\nprevious_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n\n\n1\n2024\n2023-07-09\n77\n\n\n2\n2024\n2023-07-16\n99\n\n\n3\n2024\n2023-07-23\n71\n\n\n4\n2024\n2023-07-30\n72\n\n\n\n\n\n\n\n\nprevious_weekly_performance['previous_cumulative_performance'] = (\n  previous_weekly_performance\n  .groupby('fiscal year')['previous_weekly_performance']\n  .cumsum()\n)\nprevious_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\nprevious_cumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n\n\n2\n2024\n2023-07-16\n99\n191\n\n\n3\n2024\n2023-07-23\n71\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n\n\n\n\n\n\n\n\ncurrent_weekly_performance = (\n  cleaned_df2\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'current_weekly_performance')\n)\ncurrent_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\ncurrent_weekly_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n\n\n1\n2024\n2023-07-09\n77\n\n\n2\n2024\n2023-07-16\n98\n\n\n3\n2024\n2023-07-23\n72\n\n\n4\n2024\n2023-07-30\n72\n\n\n\n\n\n\n\n\ncurrent_weekly_performance['current_cumulative_performance']= (\n  current_weekly_performance\n  .groupby('fiscal year')['current_weekly_performance']\n  .cumsum()\n)\ncurrent_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\ncurrent_weekly_performance\ncurrent_cumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n\n\n2\n2024\n2023-07-16\n98\n190\n\n\n3\n2024\n2023-07-23\n72\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n\n\n\n\n\n\n\n\nmerged_df = pd.merge(previous_weekly_performance, current_weekly_performance, how = 'outer')\nmerged_df.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\nprevious_cumulative_performance\ncurrent_weekly_performance\ncurrent_cumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n77\n92\n\n\n2\n2024\n2023-07-16\n99\n191\n98\n190\n\n\n3\n2024\n2023-07-23\n71\n262\n72\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n72\n334\n\n\n\n\n\n\n\n\nmerged_df.to_csv(\"weekly_performance.csv\", index = False)\n\n# Plotting 1\n# Set 'start date' as the index for better plotting\nmerged_df.set_index('start date', inplace=True)\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Width of the bars\nbar_width = 0.35\n\n# Positions of the bars\nindex = range(len(merged_df))\n\n# Plotting 'previous_weekly_performance'\nbars1 = ax.bar(index, merged_df['previous_weekly_performance'], width=bar_width, label='Previous Weekly Performance', color='b')\n\n# Plotting 'current_weekly_performance' shifted right by bar_width to place next to the previous\nbars2 = ax.bar([p + bar_width for p in index], merged_df['current_weekly_performance'], width=bar_width, label='Current Weekly Performance', color='r')\n\n# Adding labels, title, and legend\nax.set_xlabel('Start Date')\nax.set_ylabel('Weekly Performance')\nax.set_title('Comparison of Previous and Current Weekly Performance')\nax.legend()\n\n# Set x-ticks to be in the middle of the two bars for each date\nax.set_xticks([p + bar_width / 2 for p in index])\nax.set_xticklabels(merged_df.index.strftime('%Y-%m-%d'), rotation=45)\n\nplt.show()\n\n\n# Plotting 2\n# Resetting the index to put 'start date' back as a regular column\nmerged_df.reset_index(inplace=True)\n\n\nplt.figure(figsize=(10, 6))\n\n# Plotting the 'previous_cumulative_performance'\nplt.plot(merged_df['start date'], merged_df['previous_cumulative_performance'], label='Previous Cumulative Performance', marker='o')\n\n# Plotting the 'current_cumulative_performance'\nplt.plot(merged_df['start date'], merged_df['current_cumulative_performance'], label='Current Cumulative Performance', marker='o')\n \nplt.title('Comparison of Previous and Current Cumulative Performance Over Time')\nplt.xlabel('Start Date')\nplt.ylabel('Cumulative Performance')\nplt.legend()\nplt.grid(True)\nplt.xticks(rotation=45)  # Rotates the dates on the x-axis for better visibility\nplt.tight_layout()  # Adjusts plot parameters to give some padding and prevent overlap\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) The bar chart shows weekly performance trends for previous and current periods, highlighting fluctuations and potential seasonal patterns. This dense representation helps identify key variations but can be visually complex.\n\n\n\n\n\n\n\n\n\n\n\n(b) The line chart offers a clearer view of performance over time by smoothing out weekly noise and emphasizing long-term trends.\n\n\n\n\n\n\n\nFigure 1: Initial stages of the data visualization workflow, creating basic plots to explore and refine key trends"
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html",
    "title": "The Cost of Sunlight",
    "section": "",
    "text": "For project 2 we were instructed to “stretch our skills a little bit more”. For each project we’re allowed only 2 weeks to complete. In these past 2 weeks we’ve learned scraping, ai2html, a little D3 on top of pandas, Altair, elements of visual design and storytelling. Unfortunately, I was not able to turn my project around by the two week deadline. Though I definitely did improve from the previous project- which I completed in 3 days. Despite being more prepared I faced several roadblocks. Ai2html wouldn’t work, the svg I used as a base to work off from took a little longer to re-illustrate, and I forgot I didn’t actually know CSS that would help bring the story to life (for Project 1 I used a combination of borrowing from previous students and ChatGPT). So instead, here’s this painstaking review -a postmortem- of my process (including when I hit a wall) of Project 2."
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#the-idea",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#the-idea",
    "title": "The Cost of Sunlight",
    "section": "The Idea",
    "text": "The Idea\nWith only 2 weeks, determined not to spend too much time in the data collection phase but knowing I wanted to specifically put my scraping skills to the test, I resolved that scraping data from Zillow would be challenging enough. The discussion about rent in NYC is a topic of importance to me. But I wanted to do something a little different than what’s been discussed ad nauseam (include link about high rent in NYC). I thought about what’s important to me when it comes to looking for a place to live but is usually inaccessible or would put me in a rental bracket that’s higher than I can afford. Alas, sunlight. If you scroll through Facebook groups and craigslist posts for the apartment hopper, posts almost always (though not always the case) start with “Bright”, “North-facing” (to imply gets a lot of sunlight) or “Sunny”."
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#getting-the-data-and-narrowing-the-scope",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#getting-the-data-and-narrowing-the-scope",
    "title": "The Cost of Sunlight",
    "section": "Getting the Data and Narrowing the Scope",
    "text": "Getting the Data and Narrowing the Scope\nWith a rough idea in mind, I took to researching how exactly one could calculate or guesstimate how much sunlight an apartment would get. To my luck, the real estate company Localize.city had just what I needed. Data scientists working with the company just as aware of the value of apartment sun exposure as I was, using a combination of GIS, 3D analysis and a python library called PySolar developed an algorithm which allows them to advertise the degree of sun exposure apartments listed on their website get. I attended mentorship with Kai Teoh with what I’d found, with the intention to scrape Localize.city’s webpage. However, the site has rate blocking which quickly detects if someone (like a student practicing scraping) is scraping their website. Kai had a solution. He gave me an introduction to accessing undocumented api’s using cUrl and sent me off to test it. While I was a little disappointed that I would not get to put my new scraping skills to use I was relieved I could rely on my API skills gained 2 weeks prior and get ahead in my data processing. But I was unsuccessful and after a second check-in with Kai, we realized we definitely could not make calls to the api. Instead I would have to copy the XHR api feed for each search result page into a text file (I used Sublime) and save it as a json.\n\nWith only 1 ½ week left and determined not to relive Project 1’s experience, I started cutting, no, slashing the fat.\nInstead of all of NYC -&gt; Brookyln\nInstead of all of Brooklyn -&gt; Prospect Park (an area quadrangled by contrasting wealth groups).\nInstead of all apartment types -&gt; only those of a certain size\n\nThe resulting 13 pages weren’t ideal but were hopefully enough to tell a story or delivery insights about the relationship between sun exposure and rental prices."
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#analysis",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#analysis",
    "title": "The Cost of Sunlight",
    "section": "Analysis",
    "text": "Analysis"
  },
  {
    "objectID": "projects/footer.html",
    "href": "projects/footer.html",
    "title": "",
    "section": "",
    "text": "© 2024 Ligea Alexander. All rights reserved.\n\n\nEnjoyed this content? Share it with your network!\n\n\nEmail| GitHub | LinkedIn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ligea Alexander",
    "section": "",
    "text": "Latest Work\n\n\n\n\n\n\n\n\n\n\n\nThe Cost of Sunlight\n\n\nA unique angle on rent pricing and illuminated the impact of environmental factors on real estate.\n\n\n\n\n\n\n\n\n\n\n\n\nCellar Defenders\n\n\nExplores the world of affordable wines that deliver high quality without a hefty price tag, ideal for millennials\n\n\n\n\n\n\n\n\n\n\n\n\nManhattan is the safest place to bike in NYC. What about the rest of New York?\n\n\nDisparities in bike lane availability across NYC neighborhoods with differing ethnicities and socioeconomic statuses, demonstrate the importance of spatial equity.\n\n\n\n\n\n\nNo matching items"
  }
]