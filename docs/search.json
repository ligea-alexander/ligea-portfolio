[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ligea Alexander",
    "section": "",
    "text": "Ligea is a data analyst with 10+ years experience. She is passionate about utilizing her skills in data analysis, visualization, and storytelling to investigate and illustrate complex social, political, and economic issues.\n\n\nExperienceSkillsAdditional Skills\n\n\nFreelance | Data Journalist | 2024 - present\nFreelance | Writer | 2023 - present\nCenter for Employment Opportunities | Data Insights Specialist | 2019 - present\n\n\nArtificial Intelligence\nData Analysis\nData Visualization\nPython | pandas, numpy\nR | tidy,\nJavaScript | p5.js,\n\n\nSocial issues, community violence, gun violence, justice-involved, AI ethics\n\n\n\n\nEducation\n\n\nColumbia University | New York, NY  LEDE Program, Data Journalism| 2023\nMaryland Institute College of Art | Baltimore, MD  MPS Data Analytics and Visualization | 2020 - 2023"
  },
  {
    "objectID": "projects/blog.html",
    "href": "projects/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\nInsight through Data\n\n\n\ninsights\n\n\npython\n\n\n\n\n\n\n\n\n\n\nMay 31, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMapping with Mapbox\n\n\n\nmaps\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJun 17, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nMapping with R\n\n\n\nmaps\n\n\nR\n\n\n\n\n\n\n\n\n\n\nJun 4, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/all-work/pathway-pal/pathwaypal-chatbot.html",
    "href": "projects/all-work/pathway-pal/pathwaypal-chatbot.html",
    "title": "I Built An Enterprise ChatBot!",
    "section": "",
    "text": "Lorem ipsum Vivamus tincidunt nisi a suscipit consequat. Praesent eget convallis neque, ac scelerisque eros. Integer aliquet, elit et tempor convallis, sem libero tincidunt dolor, vitae tempor magna quam at nisi. Sed elementum ac tortor at fringilla. Aliquam et urna congue, tincidunt ipsum quis, congue nisl. Phasellus non sem massa.\nDonec quis interdum sem. Mauris ut arcu erat. Sed nisl tellus, consectetur ut tincidunt id, porttitor vel sapien. Pellentesque in lorem vitae sem euismod interdum. Phasellus fermentum consectetur velit. Duis purus diam, mattis vitae dui et, egestas tincidunt nisl. Pellentesque placerat fermentum purus eget pulvinar. Ut fringilla placerat leo, id luctus ipsum elementum non. Suspendisse accumsan metus non dolor blandit vehicula. Nulla sit amet sodales ipsum."
  },
  {
    "objectID": "projects/all-projects-2.html",
    "href": "projects/all-projects-2.html",
    "title": "All Work",
    "section": "",
    "text": "View my work in data visualization and visual storytelling.  Continue…"
  },
  {
    "objectID": "projects/all-projects-2.html#data-visualization-and-information-design",
    "href": "projects/all-projects-2.html#data-visualization-and-information-design",
    "title": "All Work",
    "section": "",
    "text": "View my work in data visualization and visual storytelling.  Continue…"
  },
  {
    "objectID": "projects/all-projects-2.html#data-journalism-and-feature-writing",
    "href": "projects/all-projects-2.html#data-journalism-and-feature-writing",
    "title": "All Work",
    "section": "Data Journalism and Feature Writing",
    "text": "Data Journalism and Feature Writing\n\n\nContinue…"
  },
  {
    "objectID": "projects/all-projects-2.html#i-swear-youre-not-stupid",
    "href": "projects/all-projects-2.html#i-swear-youre-not-stupid",
    "title": "All Work",
    "section": "I Swear, You’re Not Stupid",
    "text": "I Swear, You’re Not Stupid\n\n\nThis is a blog I started to demystify the often-intimidating world of data science, analytics, and visualization for professionals bridging the gap between academic theory and practical application. ‘I Swear, You’re Not Stupid’ offers a collection of insights, learnings, and interactive examples to not only enhance understanding but to rebuild confidence in your expertise. It’s a journey from curiosity to clarity, aimed at empowering you to navigate the complexities of data with ease and assurance. \nContinue…"
  },
  {
    "objectID": "projects/blog/corporate-visualizations/1-intervention-analysis.html",
    "href": "projects/blog/corporate-visualizations/1-intervention-analysis.html",
    "title": "",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf1 = pd.read_csv(\"anonymized_all.csv\")\ndf2 = pd.read_csv(\"anonymized_fy24_only.csv\")\n\n\ndf1.head()\n\n\n\n\n\n\n\n\nUnique Identifier\nStart Date\nClient Name\nJob ID\n\n\n\n\n0\n537926\n6/14/21\n537926 Record\njob-188418\n\n\n1\n476870\n6/18/21\n476870 Record\njob-188751\n\n\n2\n458255\n6/14/21\n458255 Record\njob-189026\n\n\n3\n479256\n8/4/20\n479256 Record\njob-181914\n\n\n4\n472697\n8/7/20\n472697 Record\njob-181916\n\n\n\n\n\n\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nUnique Identifier\nStart Date\nClient Name\nJob ID\n\n\n\n\n0\n581013\n9/25/23\n581013 Record\njob-193451\n\n\n1\n583586\n7/17/23\n583586 Record\njob-198299\n\n\n2\n585047\n7/11/23\n585047 Record\njob-198331\n\n\n3\n576444\n7/9/23\n576444 Record\njob-198365\n\n\n4\n585401\n9/18/23\n585401 Record\njob-198436\n\n\n\n\n\n\n\n\ndef clean_df(df, date_col, drop_col):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.drop([drop_col], axis = 1)\n    df.columns = df.columns.str.lower()\n    \n    return df\n\ncleaned_df1 = clean_df(df1,'Start Date', 'Client Name')\ncleaned_df2 = clean_df(df2,'Start Date', 'Client Name')\n\n\ncleaned_df1.head()\n\n\n\n\n\n\n\n\nunique identifier\nstart date\njob id\n\n\n\n\n0\n537926\n2021-06-14\njob-188418\n\n\n1\n476870\n2021-06-18\njob-188751\n\n\n2\n458255\n2021-06-14\njob-189026\n\n\n3\n479256\n2020-08-04\njob-181914\n\n\n4\n472697\n2020-08-07\njob-181916\n\n\n\n\n\n\n\n\ndef fiscal_year(date):\n    if date.month &gt;=7:\n        return date.year + 1\n    else:\n        return date.year\n    \ncleaned_df1['fiscal year'] = cleaned_df1['start date'].apply(fiscal_year)\ncleaned_df2['fiscal year'] = cleaned_df2['start date'].apply(fiscal_year)\n\n\ncleaned_df1['fiscal year'].unique()\n\narray([2021, 2022, 2023, 2024])\n\n\n\ncleaned_df2['fiscal year'].unique()\n\narray([2024])\n\n\n\n# &lt;!-- ```{python}\n# merged_df = pd.merge(cleaned_all, cleaned_fy24, how = 'outer')\n# merged_df\n# ```\n\n\n\n# ```{python}\n# merged_df['fiscal year'] = merged_df['start date'].apply(get_fiscal_year)\n# merged_df.head()\n# ```\n# ```{python}\n# merged_df['fiscal year'].unique()\n# ```\n\n# ```{python}\n# merged_df = merged_df[merged_df['fiscal year'].isin([2023, 2024])]\n# merged_df\n# ```\n\n# ```{python}\n# weekly_performance = (\n#   merged_df\n#   .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n#   .reset_index(name = 'weekly_performance')\n# )\n\n# weekly_performance\n# ```\n\n# ```{python}\n# weekly_performance['cumulative_performance'] = (\n#   weekly_performance\n#   .groupby('fiscal year')['weekly_performance']\n#   .cumsum()\n# )\n\n# weekly_performance\n# ```\n# ```{python}\n# # Pivot the data\n# weekly_performance['start date'] = weekly_performance['start date'].dt.strftime('%m-%d')\n\n# pivoted_data = weekly_performance.pivot(index='start date', columns='fiscal year', values='cumulative_performance')\n# pivoted_data.columns = [f'Cumulative Performance FY{year}' for year in pivoted_data.columns]\n\n# pivoted_data.reset_index(inplace=True)\n\n# pivoted_data\n# ```\n# ```{python}\n# # Plotting\n# fig, ax = plt.subplots(figsize=(10, 6))\n# pivoted_data.plot(x='start date', ax=ax)\n\n# ax.set_title('Cumulative Weekly Performance by Fiscal Year')\n# ax.set_xlabel('Date')\n# ax.set_ylabel('Cumulative Performance')\n# ax.legend()\n\n# plt.show()\n# ```\n# ```{python}\n# weekly_performance.to_csv(\"weekly_performance.csv\", index = False) \n# ``` --&gt;\n\n\ncleaned_previous = cleaned_df1[cleaned_df1['fiscal year'].isin([2024])]\ncleaned_previous\n\n\n\n\n\n\n\n\nunique identifier\nstart date\njob id\nfiscal year\n\n\n\n\n8381\n581013\n2023-09-25\njob-193451\n2024\n\n\n8382\n583586\n2023-07-17\njob-198299\n2024\n\n\n8383\n585047\n2023-07-11\njob-198331\n2024\n\n\n8384\n576444\n2023-07-09\njob-198365\n2024\n\n\n8385\n585401\n2023-09-18\njob-198436\n2024\n\n\n...\n...\n...\n...\n...\n\n\n11009\n619903\n2024-03-13\njob-205994\n2024\n\n\n11010\n602062\n2024-03-26\njob-206010\n2024\n\n\n11011\n598480\n2024-03-06\njob-206013\n2024\n\n\n11012\n615498\n2024-03-03\njob-206017\n2024\n\n\n11013\n460343\n2024-02-11\njob-206027\n2024\n\n\n\n\n2633 rows × 4 columns\n\n\n\n\nprevious_weekly_performance = (\n  cleaned_previous\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'previous_weekly_performance')\n)\nprevious_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n\n\n1\n2024\n2023-07-09\n77\n\n\n2\n2024\n2023-07-16\n99\n\n\n3\n2024\n2023-07-23\n71\n\n\n4\n2024\n2023-07-30\n72\n\n\n\n\n\n\n\n\nprevious_weekly_performance['previous_cumulative_performance'] = (\n  previous_weekly_performance\n  .groupby('fiscal year')['previous_weekly_performance']\n  .cumsum()\n)\nprevious_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\nprevious_cumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n\n\n2\n2024\n2023-07-16\n99\n191\n\n\n3\n2024\n2023-07-23\n71\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n\n\n\n\n\n\n\n\ncurrent_weekly_performance = (\n  cleaned_df2\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'current_weekly_performance')\n)\ncurrent_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\ncurrent_weekly_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n\n\n1\n2024\n2023-07-09\n77\n\n\n2\n2024\n2023-07-16\n98\n\n\n3\n2024\n2023-07-23\n72\n\n\n4\n2024\n2023-07-30\n72\n\n\n\n\n\n\n\n\ncurrent_weekly_performance['current_cumulative_performance']= (\n  current_weekly_performance\n  .groupby('fiscal year')['current_weekly_performance']\n  .cumsum()\n)\ncurrent_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\ncurrent_weekly_performance\ncurrent_cumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n\n\n2\n2024\n2023-07-16\n98\n190\n\n\n3\n2024\n2023-07-23\n72\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n\n\n\n\n\n\n\n\nmerged_df = pd.merge(previous_weekly_performance, current_weekly_performance, how = 'outer')\nmerged_df.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\nprevious_cumulative_performance\ncurrent_weekly_performance\ncurrent_cumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n77\n92\n\n\n2\n2024\n2023-07-16\n99\n191\n98\n190\n\n\n3\n2024\n2023-07-23\n71\n262\n72\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n72\n334\n\n\n\n\n\n\n\n\nmerged_df.to_csv(\"weekly_performance.csv\", index = False)\n\n# Plotting 1\n# Set 'start date' as the index for better plotting\nmerged_df.set_index('start date', inplace=True)\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Width of the bars\nbar_width = 0.35\n\n# Positions of the bars\nindex = range(len(merged_df))\n\n# Plotting 'previous_weekly_performance'\nbars1 = ax.bar(index, merged_df['previous_weekly_performance'], width=bar_width, label='Previous Weekly Performance', color='b')\n\n# Plotting 'current_weekly_performance' shifted right by bar_width to place next to the previous\nbars2 = ax.bar([p + bar_width for p in index], merged_df['current_weekly_performance'], width=bar_width, label='Current Weekly Performance', color='r')\n\n# Adding labels, title, and legend\nax.set_xlabel('Start Date')\nax.set_ylabel('Weekly Performance')\nax.set_title('Comparison of Previous and Current Weekly Performance')\nax.legend()\n\n# Set x-ticks to be in the middle of the two bars for each date\nax.set_xticks([p + bar_width / 2 for p in index])\nax.set_xticklabels(merged_df.index.strftime('%Y-%m-%d'), rotation=45)\n\nplt.show()\n\n\n# Plotting 2\n# Resetting the index to put 'start date' back as a regular column\nmerged_df.reset_index(inplace=True)\n\n\nplt.figure(figsize=(10, 6))\n\n# Plotting the 'previous_cumulative_performance'\nplt.plot(merged_df['start date'], merged_df['previous_cumulative_performance'], label='Previous Cumulative Performance', marker='o')\n\n# Plotting the 'current_cumulative_performance'\nplt.plot(merged_df['start date'], merged_df['current_cumulative_performance'], label='Current Cumulative Performance', marker='o')\n \nplt.title('Comparison of Previous and Current Cumulative Performance Over Time')\nplt.xlabel('Start Date')\nplt.ylabel('Cumulative Performance')\nplt.legend()\nplt.grid(True)\nplt.xticks(rotation=45)  # Rotates the dates on the x-axis for better visibility\nplt.tight_layout()  # Adjusts plot parameters to give some padding and prevent overlap\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n(a) The bar chart shows weekly performance trends for previous and current periods, highlighting fluctuations and potential seasonal patterns. This dense representation helps identify key variations but can be visually complex.\n\n\n\n\n\n\n\n\n\n\n\n(b) The line chart offers a clearer view of performance over time by smoothing out weekly noise and emphasizing long-term trends.\n\n\n\n\n\n\n\nFigure 1: Initial stages of the data visualization workflow, creating basic plots to explore and refine key trends"
  },
  {
    "objectID": "projects/blog/mapping-with-R/2.html",
    "href": "projects/blog/mapping-with-R/2.html",
    "title": "Mapping with R",
    "section": "",
    "text": "Previous Post Next Post"
  },
  {
    "objectID": "projects/all-projects.html",
    "href": "projects/all-projects.html",
    "title": "My Projects",
    "section": "",
    "text": "Below is a list of my projects:\n\n\n\n\n\n\n  \n    \n    \n      \n\n        \n        \n          \n            I Built An Enterprise ChatBot!\n          \n          \n            \n               \n            \n            \n        \n\n        \n        \n          \n            \n          \n          \n\n            \n            \n              An interactive tool that offers users answers to common questions about training schedules, services, and resources available through NEST.\n            \n\n            \n            \n              \n                \n                  \n                    Python\n                    \n                    Elasticsearch\n                    \n                    AI\n                    \n                \n                \n            \n\n            \n            \n              Read Behind The Scenes (BTS)\n            \n\n      \n    \n    \n    \n    \n      \n\n        \n        \n          \n            The Cost of Sunlight\n          \n          \n            \n               \n            \n            \n        \n\n        \n        \n          \n            \n          \n          \n\n            \n            \n              LEDE Project 2: A cost-benefit analysis of solar energy adoption.\n            \n\n            \n            \n              \n                \n                  \n                    Python\n                    \n                    Altair\n                    \n                    ai2html\n                    \n                \n                \n            \n\n            \n            \n              Read Behind The Scenes (BTS)\n            \n\n      \n    \n    \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#crafting-visual-narratives-for-corporate-strategy",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#crafting-visual-narratives-for-corporate-strategy",
    "title": "Insight through Data",
    "section": "Crafting Visual Narratives for Corporate Strategy",
    "text": "Crafting Visual Narratives for Corporate Strategy\n\n\n\n\n\n\nAll data in this case study is anonymized to protect confidentiality. Details have been altered, but the depicted processes and insights are accurately represented.\n\n\n\nIn my role as the Data Insights Specialist II on the Learning and Impact team at the Center for Employment Opportunities, I directed the overall strategy for each quarterly board report. I identified key performance indicators (KPIs) by analyzing data and aligning them with the strategic goals of the organization. I coordinated the narrative, deciding which insights to present, and shaped the story to highlight our achievements and areas for improvement. I’d communicate these insights through visualizations that were clear and impactful.\nHere, I’ll share a glimpse into how I’d transform raw data [usually obtained from various departments] into engaging visual stories that inform strategic and operational decisions. It’s all about turning complex data into clear, useful insights, even for those who aren’t familiar with data science or statistical jargon. \nEach quarter, I’d set these goals for myself:\n\nShow Org-Wide Performance: Ensure strategic alignment with fiscal year objectives.\nShare Our Strategy for Improvement: Highlight key initiatives and outcomes\nDelivering Actionable Insights: Provide clear, impactful information to the board."
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#wrangling-the-data",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#wrangling-the-data",
    "title": "Insight through Data",
    "section": "Wrangling the Data",
    "text": "Wrangling the Data\nAt the end of each quarter, I’d reach out to various departments to gather updates on programs and activities. As the Data Insights Specialist II, I stayed informed on organizational activities year-round, as our team was heavily involved in evaluations and research. Proactively engaging with pilots, program performance, evaluations, research, and analyses allowed me to know exactly what to ask for when compiling the board report, ensuring relevant insights were included.\nThe Data\nUsually, these updates came back to me as rudimentary figures, spreadsheets, or dashboards from which I’d pull data points. I would then download these into .csv files and process them using my favorite tool: pandas.\n\n\n\n\n\n\n\n\n\n\n\n(a) Via G-chat: Quick messages containing key data points or updates.\n\n\n\n\n\n\n\n\n\n\n\n(b) Through Google Sheets: More structured data in spreadsheets shared across departments.\n\n\n\n\n\n\n\n\n\n\n\n(c) Inside Emails: Often, data came embedded within the text of an email, requiring careful extraction and verification.\n\n\n\n\n\n\n\nFigure 1: Data Sharing Methods Across Departments; Illustrations by Kristin Sandgren-Jolain\n\n\n\nIn addition to incorporating relevant program updates, we also included findings from the Learning and Impact team’s internal analyses. One quarter, we included insights derived from an evaluation of one of our intervention methods. These insights revealed specific characteristics and intervention dosages that significantly increased the likelihood of success for our participants.\n“Likelihood”—there’s a term you’ll often hear in data science. Within the Learning and Impact team and in discussions across departments, terms like probabilities and likelihoods were straightforward to discuss, thanks to the context we could provide. However, translating these and other statistical terms accurately into language and visuals for the board—without the benefit of interactive dialogue—posed a unique challenge.\nIn the code blocks that follow, which I’ve anonymized for confidentiality, I’ll briefly demonstrate how I transformed data into visuals for our board updates. The data cleaning, processing, and manipulation steps shown below are routine practices I used to prepare the data points received from various departments. These steps were crucial for converting raw data, whether received through G-chat, Google Sheets, or emails, into clear, impactful board-level visual presentations.\n\n# Setting Up Environment\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Read in Data\ndf1 = pd.read_csv(\"anonymized_all.csv\")\ndf2 = pd.read_csv(\"anonymized_fy24_only.csv\")\n\n\n# First few rows of raw data\ndf1.head()\ndf2.head()\n\n\n\n\n\n\n\n\nUnique Identifier\nStart Date\nClient Name\nJob ID\n\n\n\n\n0\n581013\n9/25/23\n581013 Record\njob-193451\n\n\n1\n583586\n7/17/23\n583586 Record\njob-198299\n\n\n2\n585047\n7/11/23\n585047 Record\njob-198331\n\n\n3\n576444\n7/9/23\n576444 Record\njob-198365\n\n\n4\n585401\n9/18/23\n585401 Record\njob-198436\n\n\n\n\n\n\n\n\n# Cleaning and transforming the data\ndef clean_df(df, date_col, drop_col):\n    df[date_col] = pd.to_datetime(df[date_col])\n    df = df.drop([drop_col], axis = 1) # Drops Unnecessary Columns\n    df.columns = df.columns.str.lower() # Standardize Column Names\n    \n    return df\n\ncleaned_df1 = clean_df(df1,'Start Date', 'Client Name')\ncleaned_df2 = clean_df(df2,'Start Date', 'Client Name')\n\n\ndef fiscal_year(date): # Fiscal Year Calculation\n    if date.month &gt;=7:\n        return date.year + 1\n    else:\n        return date.year\n    \ncleaned_df1['fiscal year'] = cleaned_df1['start date'].apply(fiscal_year)\ncleaned_df2['fiscal year'] = cleaned_df2['start date'].apply(fiscal_year)\n\n\n# Check FYs in dataset\ncleaned_df1['fiscal year'].unique()\n\narray([2021, 2022, 2023, 2024])\n\n\n\ncleaned_previous = cleaned_df1[cleaned_df1['fiscal year'].isin([2024])]\n# cleaned_previous.head()\n\n\nprevious_weekly_performance = (\n  cleaned_previous\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'previous_weekly_performance')\n)\n# previous_weekly_performance\n\n\nprevious_weekly_performance['cumulative_performance'] = (\n  previous_weekly_performance\n  .groupby('fiscal year')['previous_weekly_performance']\n  .cumsum()\n)\nprevious_weekly_performance.head()\n\n\n\n\n\n\n\n\nfiscal year\nstart date\nprevious_weekly_performance\ncumulative_performance\n\n\n\n\n0\n2024\n2023-07-02\n15\n15\n\n\n1\n2024\n2023-07-09\n77\n92\n\n\n2\n2024\n2023-07-16\n99\n191\n\n\n3\n2024\n2023-07-23\n71\n262\n\n\n4\n2024\n2023-07-30\n72\n334\n\n\n\n\n\n\n\n\ncurrent_weekly_performance = (\n  cleaned_df2\n  .groupby(['fiscal year', pd.Grouper(key='start date', freq='W')]).size()\n  .reset_index(name = 'current_weekly_performance')\n)\ncurrent_weekly_performance['current_cumulative_performance']= (\n  current_weekly_performance\n  .groupby('fiscal year')['current_weekly_performance']\n  .cumsum()\n)\n# current_weekly_performance.head()\n\n\nmerged_df = pd.merge(previous_weekly_performance, current_weekly_performance, how = 'outer')\n# merged_df.head()"
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#designing-clear-visuals",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#designing-clear-visuals",
    "title": "Insight through Data",
    "section": "Designing Clear Visuals",
    "text": "Designing Clear Visuals\nAfter extensive data manipulation—cleaning, pivoting, and generating initial visuals like bar charts—I begin to see potential ways to make the data understandable for the board.\nFor the quarterly board presentation that used the anonymized code blocks introduced earlier, I constructed the two charts below using the resulting merged_df. Given the available variables, focusing on temporal dynamics and comparative metrics provides a valuable direction to uncover significant trends and irregularities to highlight in the final product.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) The bar chart shows weekly performance trends for previous and current periods, highlighting fluctuations and potential seasonal patterns. This dense representation helps identify key variations but can be visually complex.\n\n\n\n\n\n\n\n\n\n\n\n(b) The line chart offers a clearer view of performance over time by smoothing out weekly noise and emphasizing long-term trends.\n\n\n\n\n\n\n\nFigure 2: Initial stages of the data visualization workflow, creating basic plots to explore and refine key trends\n\n\n\nSource: 1-intervention-analysis.ipynb\nNow, with some initial visualizations created, I can focus on refining and enhancing the clarity of the data presentation and insight delivery. One best practice I adhere to in my visualization work is the “30-second rule” (sometimes it’s more like 2 minutes). The idea is that viewers should quickly grasp the intent of a chart or graph. This contrasts with the more abstract visualizations by my favoured artists like Federica Fragipane and Alberto Cairo, who target a different audience.\nFor board presentations, I prioritize straightforward designs and avoid overly complex or abstract visuals. Sketching with pencil and paper after extensive screen time helps me push the boundaries of basic charts just enough to enhance clarity without becoming too abstract. This physical connection with my work makes the creative process more tangible and refreshing. It allows me to identify where strategic annotations, adjustments to bar charts, or repositioning elements in a scatterplot can significantly improve the visual clarity and effectiveness.\n\n\n\n\n\n\n\n\n\nSketch 1: An early concept for communicating demographic breakdown.\n\n\n\n\n\n\n\nSketch 2: A rough layout; I use simple lines and directional arrows to indicate movement.\n\n\n\n\n\n\n\nSketch 3: When clarity and speed are key, humbling myself to use versatile bar charts is best.\n\n\n\n\n\nWith these principles in mind I refine and enhance the clarity of the data presentation to produce final visuals."
  },
  {
    "objectID": "projects/blog/corporate-visualizations/corporate-viz.html#final-product",
    "href": "projects/blog/corporate-visualizations/corporate-viz.html#final-product",
    "title": "Insight through Data",
    "section": "Final Product",
    "text": "Final Product\nBelow are some examples of the final visualizations I have created for corporate board presentations. These visuals are drawn from various projects, each tailored to address specific strategic questions. From demonstrating engagement levels across regions to highlighting the efficacy of strategic efforts over time, these visuals represent the culmination of thorough data analysis and creative design.\n\n\n\n\nAll data shown is illustrative and has been modified to maintain confidentiality.\n\n\n\n\nIntervention Visualization: This chart illustrates regional engagement levels following strategic interventions. Each number represents the number interactions per intervention type aimed at enhancing interaction effectiveness.\n\n\n\n\n\n\n\nPerformance Trend Chart The line graph depicts the effectiveness of strategic efforts, showing a notable increase in outcomes following specific interventions. Key moments, like the significant improvement marked in the chart, are pivotal discussion points in strategy meetings.\n\n\nThanks for reading :)\n\n\nNext Post"
  },
  {
    "objectID": "projects/blog/mapping-with-MapBox/3.html",
    "href": "projects/blog/mapping-with-MapBox/3.html",
    "title": "Mapping with Mapbox",
    "section": "",
    "text": "Previous Post"
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html",
    "title": "The Cost of Sunlight",
    "section": "",
    "text": "For project 2 we were instructed to “stretch our skills a little bit more”. For each project we’re allowed only 2 weeks to complete. In these past 2 weeks we’ve learned scraping, ai2html, a little D3 on top of pandas, Altair, elements of visual design and storytelling. Unfortunately, I was not able to turn my project around by the two week deadline. Though I definitely did improve from the previous project- which I completed in 3 days. Despite being more prepared I faced several roadblocks. Ai2html wouldn’t work, the svg I used as a base to work off from took a little longer to re-illustrate, and I forgot I didn’t actually know CSS that would help bring the story to life (for Project 1 I used a combination of borrowing from previous students and ChatGPT). So instead, here’s this painstaking review -a postmortem- of my process (including when I hit a wall) of Project 2."
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#the-idea",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#the-idea",
    "title": "The Cost of Sunlight",
    "section": "The Idea",
    "text": "The Idea\nWith only 2 weeks, determined not to spend too much time in the data collection phase but knowing I wanted to specifically put my scraping skills to the test, I resolved that scraping data from Zillow would be challenging enough. The discussion about rent in NYC is a topic of importance to me. But I wanted to do something a little different than what’s been discussed ad nauseam (include link about high rent in NYC). I thought about what’s important to me when it comes to looking for a place to live but is usually inaccessible or would put me in a rental bracket that’s higher than I can afford. Alas, sunlight. If you scroll through Facebook groups and craigslist posts for the apartment hopper, posts almost always (though not always the case) start with “Bright”, “North-facing” (to imply gets a lot of sunlight) or “Sunny”."
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#getting-the-data-and-narrowing-the-scope",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#getting-the-data-and-narrowing-the-scope",
    "title": "The Cost of Sunlight",
    "section": "Getting the Data and Narrowing the Scope",
    "text": "Getting the Data and Narrowing the Scope\nWith a rough idea in mind, I took to researching how exactly one could calculate or guesstimate how much sunlight an apartment would get. To my luck, the real estate company Localize.city had just what I needed. Data scientists working with the company just as aware of the value of apartment sun exposure as I was, using a combination of GIS, 3D analysis and a python library called PySolar developed an algorithm which allows them to advertise the degree of sun exposure apartments listed on their website get. I attended mentorship with Kai Teoh with what I’d found, with the intention to scrape Localize.city’s webpage. However, the site has rate blocking which quickly detects if someone (like a student practicing scraping) is scraping their website. Kai had a solution. He gave me an introduction to accessing undocumented api’s using cUrl and sent me off to test it. While I was a little disappointed that I would not get to put my new scraping skills to use I was relieved I could rely on my API skills gained 2 weeks prior and get ahead in my data processing. But I was unsuccessful and after a second check-in with Kai, we realized we definitely could not make calls to the api. Instead I would have to copy the XHR api feed for each search result page into a text file (I used Sublime) and save it as a json.\n\nWith only 1 ½ week left and determined not to relive Project 1’s experience, I started cutting, no, slashing the fat.\nInstead of all of NYC -&gt; Brookyln\nInstead of all of Brooklyn -&gt; Prospect Park (an area quadrangled by contrasting wealth groups).\nInstead of all apartment types -&gt; only those of a certain size\n\nThe resulting 13 pages weren’t ideal but were hopefully enough to tell a story or delivery insights about the relationship between sun exposure and rental prices."
  },
  {
    "objectID": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#analysis",
    "href": "projects/all-work/cost-of-sunlight/bts-cost-of-sunlight.html#analysis",
    "title": "The Cost of Sunlight",
    "section": "Analysis",
    "text": "Analysis"
  },
  {
    "objectID": "projects/footer.html",
    "href": "projects/footer.html",
    "title": "",
    "section": "",
    "text": "© 2024 Ligea Alexander. All rights reserved.\n\n\nEnjoyed this content? Share it with your network!\n\n\nEmail| GitHub | LinkedIn"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ligea Alexander",
    "section": "",
    "text": "Latest Work\n\n\n\n\n\n\n\n\n\n\n\nThe Cost of Sunlight\n\n\nA unique angle on rent pricing and illuminated the impact of environmental factors on real estate.\n\n\n\n\n\n\n\n\n\n\n\n\nCellar Defenders\n\n\nExplores the world of affordable wines that deliver high quality without a hefty price tag, ideal for millennials\n\n\n\n\n\n\n\n\n\n\n\n\nManhattan is the safest place to bike in NYC. What about the rest of New York?\n\n\nDisparities in bike lane availability across NYC neighborhoods with differing ethnicities and socioeconomic statuses, demonstrate the importance of spatial equity.\n\n\n\n\n\n\nNo matching items"
  }
]